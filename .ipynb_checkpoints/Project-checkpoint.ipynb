{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "same_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans = pd.read_csv('dataset/train_ans.csv')\n",
    "document = pd.read_csv('document.csv')\n",
    "train_query = pd.read_csv('train_query.csv')\n",
    "test_query = pd.read_csv('test_query.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_precision(df):  \n",
    "    MAP = 0\n",
    "    for query_id, doc_list_str in df.iterrows():\n",
    "        doc_list = doc_list_str[\"doc\"].split()[:50]\n",
    "        ans_doc_set = set(train_ans.loc[query_id, \"doc\"].split())\n",
    "        AP = 0\n",
    "        rel_cnt = 0\n",
    "        for i, doc in enumerate(doc_list):\n",
    "            if doc in ans_doc_set:\n",
    "                rel_cnt += 1\n",
    "                AP += rel_cnt / (i + 1)\n",
    "        AP /= min(len(ans_doc_set), 50)\n",
    "        MAP += AP\n",
    "    MAP /= len(df)\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking(max_len, sent):\n",
    "    tokenized_text = sent.lower().split(\" \")\n",
    "    # using list comprehension\n",
    "    final = [tokenized_text[i * max_len:(i + 1) *max_len] for i in range((len(tokenized_text) + max_len - 1) // max_len)] \n",
    "    \n",
    "    # join back to sentences for each of the chunks\n",
    "    sent_chunk = []\n",
    "    for item in final:\n",
    "        sent_chunk.append(' '.join(item))\n",
    "    return sent_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [13:55<00:00, 119.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# tfidf\n",
    "vectorizer = TfidfVectorizer(lowercase=True, stop_words=my_stop_words, min_df=4, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(tqdm(document['document']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:32<00:00,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.10079090144345436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result = []\n",
    "for i in tqdm(range(len(train_query['train_query']))):\n",
    "    query = train_query['train_query'][i]\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    results = cosine_similarity(X, query_vec)\n",
    "    values, indices = torch.topk(torch.tensor(results).squeeze(-1), 50)\n",
    "    r = []\n",
    "    for idxx in indices:\n",
    "        r.append(document['doc'][int(idxx)])\n",
    "    tfidf_result.append(' '.join(list(map(str, r))))\n",
    "    \n",
    "res_df = pd.DataFrame({'topic':train_ans['topic'], 'doc':tfidf_result})\n",
    "print('MAP:', mean_average_precision(res_df)) # 0.100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:10<00:00, 525.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# bm25\n",
    "tokenized_corpus = [doc.split(\" \") for doc in document['document']]\n",
    "bm25 = BM25Okapi(tqdm(tokenized_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:33<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.11097825998590742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bm_result = []\n",
    "for i in tqdm(range(len(train_query['train_query']))):\n",
    "    query = train_query['train_query'][i]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    results = bm25.get_scores(tokenized_query)\n",
    "    values, indices = torch.topk(torch.tensor(results).squeeze(-1), 50)\n",
    "    r = []\n",
    "    for idxx in indices:\n",
    "        r.append(document['doc'][int(idxx)])\n",
    "    bm_result.append(' '.join(list(map(str, r))))\n",
    "    \n",
    "bm_res_df = pd.DataFrame({'topic':train_ans['topic'], 'doc':bm_result})\n",
    "print('MAP:', mean_average_precision(bm_res_df)) # 0.1109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_output = []\n",
    "for i in range(len(tfidf_result)):\n",
    "    combine_output.append(list(set(tfidf_result[i].split()+bm_result[i].split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Query 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/86 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "combine_result = []\n",
    "for i in range(len(combine_output)):\n",
    "    r = []\n",
    "    print(f'Train Query {i+1}')\n",
    "    for j in tqdm(range(len(combine_output[i]))):\n",
    "        doc_inputs = torch.mean(torch.tensor(model.encode(chunking(300, document[document['doc'] == int(combine_output[i][j])]['document'].values[0]))), 0).tolist()\n",
    "        query_inputs = model.encode(train_query['train_query'][i]).tolist()\n",
    "        r.append(float(cosine_similarity([doc_inputs], [query_inputs])[0]))\n",
    "    values, indices = torch.topk(torch.tensor(r), 50)\n",
    "    res = []\n",
    "    for indx in indices:\n",
    "        res.append(combine_output[i][int(indx)])\n",
    "    combine_result.append(' '.join(list(map(str, res))))\n",
    "    \n",
    "res_df2 = pd.DataFrame({'topic':train_query['topic'], 'doc':combine_result})\n",
    "print()\n",
    "print('MAP:', mean_average_precision(res_df2)) # # 0.17091147408326807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_testing_result = []\n",
    "for i in tqdm(range(len(test_query['test_query']))):\n",
    "    query = test_query['test_query'][i]\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    results = cosine_similarity(X, query_vec)\n",
    "    values, indices = torch.topk(torch.tensor(results).squeeze(-1), 50)\n",
    "    r = []\n",
    "    for idxx in indices:\n",
    "        r.append(document['doc'][int(idxx)])\n",
    "    tfidf_testing_result.append(r) #' '.join(list(map(str, r))))\n",
    "\n",
    "bm_testing_result = []\n",
    "for i in tqdm(range(len(test_query['test_query']))):\n",
    "    query = test_query['test_query'][i]\n",
    "    tokenized_query = query.split(\" \")\n",
    "    results = bm25.get_scores(tokenized_query)\n",
    "    values, indices = torch.topk(torch.tensor(results).squeeze(-1), 50)\n",
    "    r = []\n",
    "    for idxx in indices:\n",
    "        r.append(document['doc'][int(idxx)])\n",
    "    bm_testing_result.append(r) #' '.join(list(map(str, r))))\n",
    "    \n",
    "testing_esemble = []\n",
    "for i in range(len(tfidf_testing_result)):\n",
    "    testing_esemble.append(list(set(tfidf_testing_result[i]+bm_testing_result[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testing_result2 = []\n",
    "for i in range(len(testing_esemble)):\n",
    "    r = []\n",
    "    print(f'Test Query {i+1}')\n",
    "    for j in tqdm(range(len(testing_esemble[i]))):\n",
    "        doc_inputs = torch.mean(torch.tensor(model.encode(chunking(300, document[document['doc'] == testing_esemble[i][j]]['document'].values[0]))), 0).tolist()\n",
    "        query_inputs = model.encode(test_query['test_query'][i]).tolist()\n",
    "        r.append(float(cosine_similarity([doc_inputs], [query_inputs])[0]))\n",
    "    values, indices = torch.topk(torch.tensor(r), 50)\n",
    "    res = []\n",
    "    for indx in indices:\n",
    "        res.append(testing_esemble[i][int(indx)])\n",
    "    testing_result2.append(' '.join(list(map(str, res))))\n",
    "    \n",
    "testing_df = pd.DataFrame({'topic':test_query['topic'], 'doc':testing_result2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.to_csv('dataset/output.csv', index=False) # 0.06358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
